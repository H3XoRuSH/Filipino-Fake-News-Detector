{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b587f4e6",
   "metadata": {},
   "source": [
    "## Filipino Fake News Detector\n",
    "\n",
    "This project was made by:\n",
    "- Justin Clyde Frongoso\n",
    "- Medwin Devilleres\n",
    "- Rae Gabriel Samonte\n",
    "- Alquen Antonio Sarmiento\n",
    "\n",
    "This project is implemented as a chrome extension tool that helps identify if an article contains fake content in the form of a paragraph, phrase or sentence through the use of the Multinomial Naive Bayes model in predicting the validity of Filipino news articles. This Jupyter notebook is made for documentation and demonstration only.\n",
    "\n",
    "The steps for the implementation can be seen below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf414dc",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries\n",
    "\n",
    "First, we import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4fb67bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import stops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a3053",
   "metadata": {},
   "source": [
    "### 2. Import Dataset\n",
    "\n",
    "We now import the dataset as well as separate the features (in this case, only the article) and the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8d155445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ayon sa TheWrap.com, naghain ng kaso si Krupa,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Kilala rin ang singer sa pagkumpas ng kanyang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>BLANTYRE, Malawi (AP) -- Bumiyahe patungong Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Kasama sa programa ang pananalangin, bulaklak ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Linisin ang Friendship Department dahil dadala...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            article\n",
       "0      0  Ayon sa TheWrap.com, naghain ng kaso si Krupa,...\n",
       "1      0  Kilala rin ang singer sa pagkumpas ng kanyang ...\n",
       "2      0  BLANTYRE, Malawi (AP) -- Bumiyahe patungong Ma...\n",
       "3      0  Kasama sa programa ang pananalangin, bulaklak ...\n",
       "4      0  Linisin ang Friendship Department dahil dadala..."
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'full.csv'\n",
    "data = pd.read_csv(path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "205aa15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1603\n",
       "1    1603\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e9507be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['article']\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "846fb746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Article Length: 365.45290081097943\n"
     ]
    }
   ],
   "source": [
    "lens = 0\n",
    "for i in x:\n",
    "    cur_len = len(i.split())\n",
    "    lens += cur_len\n",
    "\n",
    "print(f\"Average Article Length: {lens / 1603}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d176368b",
   "metadata": {},
   "source": [
    "### 3. Splitting the Dataset (for training and testing)\n",
    "\n",
    "The data will now be splitted into two: training set and test set. Since there are only 3000+ rows, we are splitting the data in this way: 80% training and 20% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d713fc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2564,)\n",
      "(642,)\n",
      "(2564,)\n",
      "(642,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99a1c7",
   "metadata": {},
   "source": [
    "### 4. Vectorizing the Dataset\n",
    "\n",
    "Now, we need to vectorize the dataset to process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "216ebee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<642x34687 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 51383 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words = [i for i in stops.stop_words])\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fa231",
   "metadata": {},
   "source": [
    "### 5. Building the Model\n",
    "\n",
    "We are using the Multinomial Naive Bayes Classifier as it is suitable for classification of discrete features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b10c450e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d15298d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "aa413909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9330218068535826"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4484e8b9",
   "metadata": {},
   "source": [
    "### 6. Predicting Text\n",
    "\n",
    "Now that we trained the model, it is now time to use it and predict some pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4aba9add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wala pa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
